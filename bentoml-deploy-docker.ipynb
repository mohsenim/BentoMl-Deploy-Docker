{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bb5177-9089-4aa2-ab57-6c6ce805a2ff",
   "metadata": {},
   "source": [
    "# Serving and Deploying Machine Learning Models with BentoML: Germany Car Price Prediction Case Study\n",
    "\n",
    "In the [this](https://medium.com/@mohsenim/tracking-machine-learning-experiments-with-mlflow-and-dockerizing-trained-models-germany-car-price-e539303b6f97) post and [this](https://github.com/mohsenim/MLflow-XGBoost-Docker) GitHub repository, I discussed [MLflow](https://mlflow.org/) and its capabilities for tracking machine learning (ML) experiments, and serving and containerizing ML models. In this post, I will explore [BentoML](https://www.bentoml.com/), another open-source platform for packaging ML models. While BentoML may not offer the same level of experiment tracking and logging features as MLflow, it effectively simplifies the productionization of ML models by offering easy-to-use built-in features. Instead of comparing BentoML to MLflow, this post will focus solely on BentoML. I'll save the comparison between the two platforms for a future post.\n",
    "\n",
    "BentoML provides a straightforward approach for packaging trained models and their associated code and dependencies into a unified distribution format called \"Bento\". This platform supports various deployment options, including serving models as REST APIs, Docker containers, and batch jobs. \n",
    "\n",
    "As in the [previous](https://medium.com/@mohsenim/tracking-machine-learning-experiments-with-mlflow-and-dockerizing-trained-models-germany-car-price-e539303b6f97) post, we focus on predicting car prices in Germany as a case study. To accomplish this, we use the [Germany Cars Dataset](https://www.kaggle.com/datasets/ander289386/cars-germany), which was previously discussed in terms of its preprocessing and cleaning procedures. We establish a pipeline comprising a preprocessing stage for data preparation, followed by the implementation of an XGBoost model. It's worth noting that alternative models can be seamlessly integrated. XGBoost was selected for its efficiency and widespread popularity as a machine learning ensemble algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a98c6-ce15-459d-9a70-859d96b753ca",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "First, we import the necessary libraries for reading the dataset, training, and serving a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95773ee-8bb8-4eb6-ae14-ecc5b38ce157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import bentoml\n",
    "from typing import Annotated\n",
    "from bentoml.validators import DataframeSchema\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543dbf21-c963-47e6-9203-133c80b4ee74",
   "metadata": {},
   "source": [
    "## Train a Model\n",
    "\n",
    "In this section, we begin by defining a pipeline for preprocessing categorical variables and integrating an XGBoost model into the pipeline. We employ XGBoost, a highly potent machine learning model known for its effectiveness. XGBoost offers a range of parameters; for detailed information about these parameters, please refer to the [XGBoost documentation](https://xgboost.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372017e6-abca-4bab-ac3b-dfb26c0a3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_model_pipeline(categorical_cols, params):\n",
    "    \"\"\"\n",
    "    Build the model\n",
    "    \"\"\"\n",
    "    ordinal_encoder = preprocessing.OrdinalEncoder()\n",
    "    preprocess = ColumnTransformer(\n",
    "        [(\"Ordinal-Encoder\", ordinal_encoder, categorical_cols)],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    xgb_model = xgb.XGBRegressor(**params)\n",
    "    pipeline = Pipeline([(\"preprocess\", preprocess), (\"xgb_model\", xgb_model)])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59c006-aedc-4951-bcf9-ce5b1659967c",
   "metadata": {},
   "source": [
    "Now, we define a function to load the dataset and another function to train a model. Feel free to adjust the parameters of XGBoost as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75612d2-6dbf-4a11-969c-f3d6f30e23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Load the datase (csv file) from `path`\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    categorical_cols = [\"make\", \"model\", \"fuel\", \"gear\", \"offerType\"]\n",
    "    numerical_cols = [\"mileage_log\", \"hp\", \"age\", \"price_log\"]\n",
    "\n",
    "    cols = categorical_cols + numerical_cols\n",
    "    data = df[cols]\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.20, random_state=37)\n",
    "    train_x = train.drop([\"price_log\"], axis=1)\n",
    "    train_y = train[[\"price_log\"]]\n",
    "\n",
    "    test_x = test.drop([\"price_log\"], axis=1)\n",
    "    test_y = test[[\"price_log\"]]\n",
    "    return (\n",
    "        train_x,\n",
    "        train_y,\n",
    "        test_x,\n",
    "        test_y,\n",
    "        categorical_cols,\n",
    "        numerical_cols,\n",
    "    )\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    dataset_path = Path(\"./data/autoscout24-germany-dataset-cleaned.csv\")\n",
    "    train_x, train_y, test_x, test_y, categorical_cols, _ = load_dataset(dataset_path)\n",
    "\n",
    "    params = {\"max_depth\": 8, \"subsample\": 0.7}\n",
    "\n",
    "    pipeline = get_xgb_model_pipeline(categorical_cols=categorical_cols, params=params)\n",
    "    pipeline.fit(train_x, train_y)\n",
    "\n",
    "    # Evaluation\n",
    "    pred_y = pipeline.predict(test_x)\n",
    "    eval_metric = mean_squared_error(test_y, pred_y)\n",
    "\n",
    "    result = {\"mse\": eval_metric, \"model\": pipeline}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df624a93-b688-4d09-b883-386107be5852",
   "metadata": {},
   "source": [
    "Everything is now ready to train a model and save it for further use. It is worth mentioning that the model predicts prices in logarithmic scale, and this should be taken into account when interpreting model evaluation metrics and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc6b23f2-f851-44ad-8ea5-fa2180745364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained! Mean squared error (MSE) of the model: 0.003412174771345764\n",
      "Model german_car_model.pkl is saved in: 'artifacts'.\n"
     ]
    }
   ],
   "source": [
    "artifacts_path = Path(\"./artifacts\")\n",
    "\n",
    "# train a model\n",
    "result = train()\n",
    "print(f\"Trained! Mean squared error (MSE) of the model: {result['mse']}\")\n",
    "\n",
    "# save the model\n",
    "model_name = \"german_car_model.pkl\"\n",
    "joblib.dump(result[\"model\"], artifacts_path / model_name)\n",
    "print(f\"Model {model_name} is saved in: '{artifacts_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273bbb4-6d5d-47b0-a1be-e72670e34c6f",
   "metadata": {},
   "source": [
    "## Create a Service\n",
    "\n",
    "The true power of BentoML lies in its simplified approach to serving a model as REST APIs. It supports various data types, which enable it to validate the data flowing into and out of a service. In the code below, we load the model saved in the `artifacts` folder and define the service endpoint `predict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c96495b-91f0-48ed-957e-0eb2830ed3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_path = Path('./artifacts')   \n",
    "\n",
    "@bentoml.service(\n",
    "    resources={\"cpu\": \"1\"},\n",
    "    traffic={\"timeout\": 10},\n",
    ")\n",
    "class CarPricePrediction:\n",
    "    def __init__(self) -> None:\n",
    "        self.pipeline = joblib.load(artifacts_path/\"car_price_model.pkl\")\n",
    "\n",
    "    @bentoml.api\n",
    "    def predict(self, \n",
    "                input_records: Annotated[pd.DataFrame, DataframeSchema(orient='records', columns=[\"make\",\"model\",\"fuel\",\"gear\",\"offerType\",\"mileage_log\",\"hp\",\"age\"])]\n",
    "               ) -> np.ndarray:\n",
    "        result = self.pipeline.predict(input_records)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f872ee3-2004-465c-81b1-71559a317d84",
   "metadata": {},
   "source": [
    "## Serve the Model\n",
    "\n",
    "The following command makes the service accessible at the address `http://127.0.0.1:3000/predict`:\n",
    "```\n",
    "bentoml serve service:CarPricePrediction\n",
    "```\n",
    "\n",
    "To test the model, requests can be sent to the REST API using the curl command:\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "  'http://127.0.0.1:3000/predict' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"input_records\": [\n",
    "    {\"make\":\"Skoda\", \"model\": \"Fabia\", \"fuel\": \"Gasoline\" ,\"gear\": \"Automatic\", \"offerType\": \"Used\",\"mileage_log\": 1.929419,\"hp\": 95.0 ,\"age\": 0.0}\n",
    "  ]\n",
    "}'\n",
    "```\n",
    "\n",
    "which returns the following result:\n",
    "```\n",
    "[4.235708236694336]\n",
    "```\n",
    "\n",
    "As the model is trained to predict the logarithm of price, the output is in logarithmic scale and should be converted back using `10**prediction`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49890563-e6ff-46e5-a639-652509c37aa2",
   "metadata": {},
   "source": [
    "## Build the Bento and Deploy the Model\n",
    "\n",
    "To build a Bento, which is a format defining all components required for running a BentoML service, a YAML configuration file should be prepared. This file which is by default named `bentofile.yaml` is like the following in our case:  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5cacf15-12df-4e52-a587-110723fb477f",
   "metadata": {},
   "source": [
    "service: 'service:CarPricePrediction'\n",
    "description: |\n",
    "  An XGBoost model for Germany car price prediction\n",
    "labels:\n",
    "  owner: mohsenim\n",
    "  project: Germany Car Price Prediction\n",
    "include:\n",
    "  - \"**/*.py\"\n",
    "  - \"artifacts/*\"\n",
    "  - \"data/*\"\n",
    "  - \"**/bentoml*\"\n",
    "python:\n",
    "  packages:\n",
    "    - numpy==1.26.4\n",
    "    - pandas==2.2.2\n",
    "    - scikit_learn==1.4.2\n",
    "    - xgboost==2.0.3\n",
    "docker:\n",
    "  distro: debian\n",
    "  python_version: \"3.10\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8089e0c2-0121-4103-a904-a9192465b9f4",
   "metadata": {},
   "source": [
    "Now, we can build the Bento by running:\n",
    "```\n",
    "bentoml build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0070b9d-2e04-4a70-b4ce-06d4ea668254",
   "metadata": {},
   "source": [
    "\n",
    "After building the Bento, a docker image can be created from the Bento:\n",
    "```\n",
    "bentoml containerize service:CarPricePrediction\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2bd9d-2d01-4893-b9f4-2f03664243a6",
   "metadata": {},
   "source": [
    "## Running the Docker Image\n",
    "\n",
    "The above command creates a docker image called `car_price_prediction:7tfbadqjpkt6coaa`. The image can later be run locally, on a server, or on a cloud. To run the docker image locally, you can use this command:\n",
    "```\n",
    "docker run --rm -p 3000:3000 car_price_prediction:7tfbadqjpkt6coaa\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
